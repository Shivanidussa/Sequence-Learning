{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9df2b86",
   "metadata": {},
   "source": [
    "     Transformer architecture has encoder and decoder stack, hence called encoder-decoder architecture whereas BERT is just an encoder stack of transformer architecture. There are two variants, BERT-base and BERT-large, which differ in architecture complexity. The base model has 12 layers in the encoder whereas the Large has 24 layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0e0855",
   "metadata": {},
   "source": [
    "       BERT was trained on a large text corpus, which gives architecture/model the ability to better understand the language and to learn variability in data patterns and generalizes well on several NLP tasks. As it is bidirectional that means BERT learns information from both the left and the right side of a token’s context during the training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b57d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bca5118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ab93b",
   "metadata": {},
   "source": [
    "### Load IMDB Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2910f364",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews', split = (tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "                                          as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2b073a1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='imdb_reviews',\n",
       "    full_name='imdb_reviews/plain_text/1.0.0',\n",
       "    description=\"\"\"\n",
       "    Large Movie Review Dataset.\n",
       "    This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\n",
       "    \"\"\",\n",
       "    config_description=\"\"\"\n",
       "    Plain text\n",
       "    \"\"\",\n",
       "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
       "    data_path='C:\\\\Users\\\\Shivani Dussa\\\\tensorflow_datasets\\\\imdb_reviews\\\\plain_text\\\\1.0.0',\n",
       "    file_format=tfrecord,\n",
       "    download_size=80.23 MiB,\n",
       "    dataset_size=129.83 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "        'text': Text(shape=(), dtype=tf.string),\n",
       "    }),\n",
       "    supervised_keys=('text', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
       "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
       "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
       "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
       "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
       "      month     = {June},\n",
       "      year      = {2011},\n",
       "      address   = {Portland, Oregon, USA},\n",
       "      publisher = {Association for Computational Linguistics},\n",
       "      pages     = {142--150},\n",
       "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84461dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensorflow.python.data.ops.dataset_ops.PrefetchDataset,\n",
       " tensorflow.python.data.ops.dataset_ops.PrefetchDataset)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds_train),type(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "695b2e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((), ()), types: (tf.string, tf.int64)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60576293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_train), len(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e40f29",
   "metadata": {},
   "source": [
    "### Explore IMDB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46c02425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.PrefetchDataset"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df0f9ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_datasets.core.dataset_utils._IterableDataset at 0x21ba0759670>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.as_numpy(ds_train.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6973541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", 0)\n",
      "(b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.', 0)\n",
      "(b'Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.', 0)\n",
      "(b'This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received.', 1)\n",
      "(b'As others have mentioned, all the women that go nude in this film are mostly absolutely gorgeous. The plot very ably shows the hypocrisy of the female libido. When men are around they want to be pursued, but when no \"men\" are around, they become the pursuers of a 14 year old boy. And the boy becomes a man really fast (we should all be so lucky at this age!). He then gets up the courage to pursue his true love.', 1)\n"
     ]
    }
   ],
   "source": [
    "for data in tfds.as_numpy(ds_train.take(5)):\n",
    "    print(data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087fbd1f",
   "metadata": {},
   "source": [
    "##### Observation:\n",
    "          - Here we taken first 5 reviews from imdb reviews data.. after the sentence it also given whether the sentence is positive(1) or negitive(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6328803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "type: <class 'bytes'>\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "0\n",
      "type: <class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# here, we are dividing that review sentence and label whether the sentence is +ve or -ve\n",
    "for review, label in tfds.as_numpy(ds_train.take(1)):\n",
    "    print(review)\n",
    "    print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "    print(\"type:\",type(review))\n",
    "    print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "    print(label)\n",
    "    print(\"type:\",type(label))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1949dba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was an absolutely terrible movie. Don't be lured in \t 0\n",
      "I have been known to fall asleep during films, but this  \t 0\n",
      "Mann photographs the Alberta Rocky Mountains in a superb \t 0\n",
      "This is the kind of film for a snowy Sunday afternoon wh \t 1\n",
      "As others have mentioned, all the women that go nude in  \t 1\n",
      "This is a film which should be seen by anybody intereste \t 1\n"
     ]
    }
   ],
   "source": [
    "reviews = []\n",
    "labels= []\n",
    "for review, label in tfds.as_numpy(ds_train.take(6)):    # taking 6 reviews i.e., 0-5\n",
    "    reviews.append(review.decode())\n",
    "    labels.append(label)\n",
    "    print(review.decode()[0:56], '\\t', label)     # this is 0:50 means it takes sentences of words space also means to a 1 word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114fbe2",
   "metadata": {},
   "source": [
    "**Here, 0,1,2 indexes sentences are negitive and 3,4 indexes are positive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48eb848c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ead33f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\",'distilbert-base-uncased-finetuned-sst-2-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91f53bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.999795138835907}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.9992781281471252}]\n",
      "[{'label': 'POSITIVE', 'score': 0.664378821849823}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9997604489326477}]\n",
      "[{'label': 'POSITIVE', 'score': 0.999556839466095}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9994988441467285}]\n"
     ]
    }
   ],
   "source": [
    "print(classifier(reviews[0]))   # predicts correctly 0.99\n",
    "print(classifier(reviews[1]))   # predicts correctly 0.99\n",
    "print(classifier(reviews[2]))   # predicts wrongly 0.66 its a negitive sentence but its shows positive with 0.66 accuracy score\n",
    "print(classifier(reviews[3]))   # predicts correctly 0.99\n",
    "print(classifier(reviews[4]))   # predicts correctly 0.99\n",
    "print(classifier(reviews[5]))   # predicts correctly 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680b2429",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "Let’s prepare the data according to the format needed for the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21e2c2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62134ccc5fa4f23ae22d13649a508d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Shivani Dussa\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08e08747fb24bd0b3914c12eed3c3ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20b92bacc574df0994f1a9ec27fa53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "faab69d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be upto 512  for berts\n",
    "max_length = 512\n",
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65d01a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_feature(review):\n",
    "    return tokenizer.encode_plus(review,add_special_tokens= True,    # add [CLS] [SEP]\n",
    "                                max_length = max_length,             #max length of the text that can go to BERT\n",
    "                                pad_to_max_length = True,            #add [PAD] tokens\n",
    "                                return_attention_mask= True,         #add atention mask to not focus on pad tokens\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "95e14546",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "[101, 2023, 2001, 2019, 7078, 6659, 3185, 1012, 2123, 1005, 1056, 2022, 26673, 1999, 2011, 5696, 3328, 2368, 2030, 2745, 3707, 7363, 1012, 2119, 2024, 2307, 5889, 1010, 2021, 2023, 2442, 3432, 2022, 2037, 5409, 2535, 1999, 2381, 1012, 2130, 2037, 2307, 3772, 2071, 2025, 2417, 21564, 2023, 3185, 1005, 1055, 9951, 9994, 1012, 2023, 3185, 2003, 2019, 2220, 3157, 7368, 2149, 10398, 3538, 1012, 1996, 2087, 17203, 5019, 2020, 2216, 2043, 1996, 25882, 8431, 2020, 2437, 2037, 3572, 2005, 25239, 1012, 3814, 9530, 5428, 2696, 17649, 2596, 6887, 16585, 1010, 1998, 2014, 18404, 1011, 2293, 6771, 2007, 3328, 2368, 2001, 2498, 2021, 1037, 17203, 6832, 13354, 1999, 1037, 3185, 2008, 2001, 22808, 1997, 2151, 2613, 3574, 1012, 1045, 2572, 9364, 2008, 2045, 2024, 5691, 2066, 2023, 1010, 27853, 3364, 1005, 1055, 2066, 5696, 3328, 2368, 1005, 1055, 2204, 2171, 1012, 1045, 2071, 4510, 4133, 2083, 2009, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for review, label in tfds.as_numpy(ds_train.take(1)):\n",
    "    encodedReview = convert_example_to_feature(review.decode())\n",
    "    print(\"type:\",type(encodedReview))\n",
    "    print(\"----------------------------------------------------------------------------------------------------------------\")\n",
    "    print(\"keys:\",encodedReview.keys())\n",
    "    print(\"----------------------------------------------------------------------------------------------------------------\")\n",
    "    #print(encodedReview)\n",
    "    print(encodedReview['input_ids'])\n",
    "    print(\"----------------------------------------------------------------------------------------------------------------\")\n",
    "    print(encodedReview['token_type_ids'])\n",
    "    print(\"----------------------------------------------------------------------------------------------------------------\")\n",
    "    print(encodedReview['attention_mask'])\n",
    "    print(\"----------------------------------------------------------------------------------------------------------------\")\n",
    "    print(review.decode())\n",
    "    print(\"----------------------------------------------------------------------------------------------------------------\")\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67076ba",
   "metadata": {},
   "source": [
    "##### Input IDs:   \n",
    "          The input ids are often the only required parameters to be passed to the model as input. Token indices, numerical representations of tokens building the sequences that will be used as input by the model.\n",
    "\n",
    "##### Attention mask:\n",
    "          Attention Mask is used to avoid performing attention on padding token indices. Mask value can be either 0 or 1, 1 for tokens that are NOT MASKED, 0 for MASKED tokens.\n",
    "\n",
    "##### Token type ids: \n",
    "         It is used in use cases like sequence classification or question answering. As these require two different sequences to be encoded in the same input IDs. Special tokens, such as the classifier[CLS] and separator[SEP] tokens are used to separate the sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a9781",
   "metadata": {},
   "source": [
    "### Encode Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6c26d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is helper function to transform our raw data to an appropriate format ready to feed into the BERT model\n",
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "    return {\n",
    "        \"input_ids\":input_ids,\n",
    "        \"token_type_ids\":token_type_ids,\n",
    "        \"attention_mask\":attention_masks,\n",
    "    }, label\n",
    "\n",
    "def encode_examples(ds, limit = -1):\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    if (limit >0):\n",
    "        print(\"Using\", limit, \"records from ds\")\n",
    "        ds = ds.take(limit)\n",
    "    else:\n",
    "        print(\"Using all records from ds\")\n",
    "    for review, label in tfds.as_numpy(ds):\n",
    "        bert_input = tokenizer.encode_plus(review.decode(),add_special_tokens = True,max_length = max_length,\n",
    "                                           pad_to_max_length = True,return_attention_mask = True,)\n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list,attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c08258f",
   "metadata": {},
   "source": [
    "- **batch** - Combines consecutive elements of this dataset into batch\n",
    "- **shuffle** - Randomly shuffles the elements of this dataset. This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7126ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 5000 records from ds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1000 records from ds\n"
     ]
    }
   ],
   "source": [
    "# train dataset\n",
    "ds_train_encoded = encode_examples(ds_train, limit = 5000).shuffle(2000).batch(batch_size)\n",
    "# test dataset\n",
    "ds_test_encoded = encode_examples(ds_test, limit = 1000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2581d6c4",
   "metadata": {},
   "source": [
    "### Understanding ds_train encoded structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3692b3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "for record in ds_train_encoded.take(1).as_numpy_iterator():\n",
    "    print(type(record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31535b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb371354",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': array([[  101,  2013,  1996, ...,  1997, 15587,   102],\n",
       "         [  101,  2092,  1010, ...,     0,     0,     0],\n",
       "         [  101,  1996,  2034, ...,     0,     0,     0],\n",
       "         [  101,  7004, 13573, ...,     0,     0,     0],\n",
       "         [  101,  1045,  2387, ...,     0,     0,     0],\n",
       "         [  101,  1045,  2074, ...,     0,     0,     0]]),\n",
       "  'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]),\n",
       "  'attention_mask': array([[1, 1, 1, ..., 1, 1, 1],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0]])},\n",
       " array([[1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0]]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f769ef91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# here, we are separating as inputs,token_ids,attention mask as inputs and label as labels \n",
    "for inputs, labels in ds_train_encoded.take(1).as_numpy_iterator():\n",
    "    print(type(inputs))\n",
    "    print(type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "33cf3891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7e1c7ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b2103c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in ds_train_encoded.take(1).as_numpy_iterator():\n",
    "    print(inputs.keys())\n",
    "    print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "29e0282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in ds_train_encoded.take(1).as_numpy_iterator():\n",
    "    print(len(inputs['input_ids']))\n",
    "    print(len(inputs['token_type_ids']))\n",
    "    print(len(inputs['attention_mask']))\n",
    "    print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2cc25aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4a6ba573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e0bfecc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 101, 2178, 2028, ...,    0,    0,    0],\n",
       "       [ 101, 1045, 2074, ...,    0,    0,    0],\n",
       "       [ 101, 1045, 6719, ...,    0,    0,    0],\n",
       "       [ 101, 1999, 2344, ...,    0,    0,    0],\n",
       "       [ 101, 1045, 3427, ...,    0,    0,    0],\n",
       "       [ 101, 2045, 2003, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7034a1e",
   "metadata": {},
   "source": [
    "**As we have selected batch_size=6, we see each data record consists of 6 encoded reviews & 6 labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c4c7c855",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101  1996 13130  7065 16116  2105  1037  2611  2040  2442  4553  2000\n",
      "  2022  3625  2005  2014  2219  4506  1012  2004  2016  2018  1996  2373\n",
      "  1997  3894  1010  2016  2411  2109  2009  2000  3046  2000  2393  2014\n",
      "  3866  3924  2030  2841  1010  4703  4525  1999 18204 26136  2015  2008\n",
      "  2024  2411 16775  1998  2467 14742  1012  1026  7987  1013  1028  1026\n",
      "  7987  1013  1028  1996  2565  2211  2007 21876  1005  1055  7357  1999\n",
      "  2152  2082  1999  1996  7214  2237  1997  2225  6374  1010  2284  2379\n",
      "  3731  1010  4404  1006  2004  4941  2000  2665  5634  1999  1996  5888\n",
      "  1007  1012  1999  1996  2186  1005  2101  3692  1010 21876  3852  2013\n",
      "  2152  2082  1998  8302  1999  2267  1010  2059  2333  2006  2000  2014\n",
      "  4740  2000  2444  2006  2014  2219  1998  2562  1037  3105  2012  1996\n",
      "  2334  3780  1012  4911  2582  2013  2049  5021  6147  1010  1996  2265\n",
      "  3092  2007 21876  1005  1055  5030  1010  2348  1010  1999  1996  2203\n",
      "  1010  2016  4704  1996  5030  1998  2743  2125  2007  7702  1012  1026\n",
      "  7987  1013  1028  1026  7987  1013  1028  2116  4178  9125 21876  2893\n",
      "  2000  3113  1010  2083  3019  2030 11189  2965  1010  2759  2613  1011\n",
      "  2166  3315  3324  1997  1996  2051  1010  2164  4658  3695  1010  1996\n",
      "  6355 26893  2015  1010  1996 10457 13334  2102  3337  1010 11588  4774\n",
      "  1010 23255  3557  1997  1996  8284 10285  1010 29168 13957  1010 20704\n",
      " 15928  2474  5737 10177  1010  3817  2793  2075  3790  1010 17179  1010\n",
      " 10267  1005  1055 10188  1010  9576  3871  1010  1005  1050 26351  1010\n",
      "  1998 24595 16778  1012  2607  1997  3267  1010  1996  2316  1997  9606\n",
      "  7437  7530  1005  1055  2059  1011  6898  1006  2085  3129  1007  2928\n",
      " 19863 11451  2239  1010  2596  1999  2019  2792  1999  2526  1012   102\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in ds_train_encoded.take(1).as_numpy_iterator():\n",
    "    print(inputs['input_ids'][0])\n",
    "    print(inputs['token_type_ids'][0])\n",
    "    print(inputs['attention_mask'][0])\n",
    "    print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e427b9",
   "metadata": {},
   "source": [
    "### Link to Google Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ed2beb08",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [80]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://drive.google.com/drive/my-drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mln -s /content/gdrive/My\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m Drive/ /mydrive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('https://drive.google.com/drive/my-drive')\n",
    "!ln -s /content/gdrive/My\\ Drive/ /mydrive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b61e3e8",
   "metadata": {},
   "source": [
    "### Creating and Training Fine Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8432fec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b56a96a30d24d63adf48548da2ff0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/536M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "learning_rate = 2e-5  #recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "number_of_epochs = 1 # taking 1 epoch \n",
    "\n",
    "#model initializational\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0e43b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing Adam Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate, epsilon = 1e-08)\n",
    "# we do not have one-hot vectors, here we can use sparse categorical crossentrophy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer = optimizer, loss = loss, metrics = [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a360b619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12/834 [..............................] - ETA: 11:09:41 - loss: 0.7013 - accuracy: 0.5417"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_history = model.fit(ds_train_encoded, epochs = number_of_epochs, validation_data = ds_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "cbff878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n"
     ]
    }
   ],
   "source": [
    "!copy /mydrive/fineTuneBERTwithIMDB_weights.* /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "150d6aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('fineTuneBERTwithIMDB_weights.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6976c09",
   "metadata": {},
   "source": [
    "### Use previously Trained weights(instead of training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "89464aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x21bd23dbee0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "model.load_weights('fineTuneBERTwithIMDB_weights.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52497140",
   "metadata": {},
   "source": [
    "### Inference on random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fec51af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "myreview = \"Though the movie was good, it was a bit too long\"\n",
    "encoded_myreview = tokenizer.encode(myreview, truncation=True, padding=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a8524b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "tf_output = model.predict(encoded_myreview)[0]\n",
    "tf_prediction = tf.nn.softmax(tf_output,axis = 1)\n",
    "labels = ['Negative','Positive'] #(0:negative, 1:positive)\n",
    "label = tf.argmax(tf_prediction, axis = 1)\n",
    "label = label.numpy()\n",
    "print(labels[label[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "67cf0e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16335757 0.14577296]]\n",
      "tf.Tensor([[0.504396   0.49560395]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf_output)\n",
    "print(tf_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1363819",
   "metadata": {},
   "source": [
    "### Inference on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fc2b7385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are films that make careers. For George Rome \t 1\n",
      "A blackly comic tale of a down-trodden priest, Naz \t 1\n",
      "Scary Movie 1-4, Epic Movie, Date Movie, Meet the  \t 0\n",
      "Poor Shirley MacLaine tries hard to lend some grav \t 0\n",
      "As a former Erasmus student I enjoyed this film ve \t 1\n"
     ]
    }
   ],
   "source": [
    "reviews = []\n",
    "labels = []\n",
    "for review, label in tfds.as_numpy(ds_test.take(5)):\n",
    "    reviews.append(review.decode())\n",
    "    labels.append(label)\n",
    "    print(review.decode()[0:50], '\\t', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ecdfabe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come.\""
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "reviews[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "57966f2b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A blackly comic tale of a down-trodden priest, Nazarin showcases the economy that Luis Bunuel was able to achieve in being able to tell a deeply humanist fable with a minimum of fuss. As an output from his Mexican era of film making, it was an invaluable talent to possess, with little money and extremely tight schedules. Nazarin, however, surpasses many of Bunuel's previous Mexican films in terms of the acting (Francisco Rabal is excellent), narrative and theme.<br /><br />The theme, interestingly, is something that was explored again in Viridiana, made three years later in Spain. It concerns the individual's struggle for humanity and altruism amongst a society that rejects any notion of virtue. Father Nazarin, however, is portrayed more sympathetically than Sister Viridiana. Whereas the latter seems to choose charity because she wishes to atone for her (perceived) sins, Nazarin's whole existence and reason for being seems to be to help others, whether they (or we) like it or not. The film's last scenes, in which he casts doubt on his behaviour and, in a split second, has to choose between the life he has been leading or the conventional life that is expected of a priest, are so emotional because they concern his moral integrity and we are never quite sure whether it remains intact or not.<br /><br />This is a remarkable film and I would urge anyone interested in classic cinema to seek it out. It is one of Bunuel's most moving films, and encapsulates many of his obsessions: frustrated desire, mad love, religious hypocrisy etc. In my view 'Nazarin' is second only to 'The Exterminating Angel', in terms of his Mexican movies, and is certainly near the top of the list of Bunuel's total filmic output.\""
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "reviews[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cbcd0c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "myreview = reviews[i]\n",
    "encoded_myreview = tokenizer.encode(myreview, truncation = True, padding = True, return_tensors = \"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ae7f8ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021BD1CFA550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021BD1CFA550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "tf_output = model.predict(encoded_myreview)[0]\n",
    "tf_prediction = tf.nn.softmax(tf_output, axis=1)\n",
    "labels = ['Negative','Positive'] #(0:negative, 1:positive)\n",
    "label = tf.argmax(tf_prediction, axis=1)\n",
    "label = label.numpy()\n",
    "print(labels[label[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9be3f5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.097545   0.05821977]]\n",
      "tf.Tensor([[0.50983006 0.49017   ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf_output)\n",
    "print(tf_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3e54344b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9988347887992859}]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(myreview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd4bd5e",
   "metadata": {},
   "source": [
    "tokenizer. encode will encode our test example into integers using Bert tokenizer, then we use predict method on the encoded input to get our predictions. The model. predict will return logits, on which we can apply softmax function to get the probabilities for each class, and then using TensorFlow argmax function we can get the class with the highest probability and map it to text labels (positive or negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6eb8e2",
   "metadata": {},
   "source": [
    "### Extra code -- to understand how tf.data.Dataset.from_tensor_slices() works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6905b60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "for element in dataset:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "aa53ee88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "dataset = dataset.map(lambda x: x*2)\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "03991bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "print(list(dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "366de61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),\n",
    "                                              'b': [5, 6]})\n",
    "list(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},\n",
    "                                      {'a': (2, 4), 'b': 6}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6f958490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': (1, 3), 'b': 5}, {'a': (2, 4), 'b': 6}]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset.as_numpy_iterator())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
