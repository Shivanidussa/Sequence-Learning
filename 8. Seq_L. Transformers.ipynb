{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b055562",
   "metadata": {},
   "source": [
    "# Transformers\n",
    " -**What they can do?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38522df6",
   "metadata": {},
   "source": [
    "https://huggingface.co/course/chapter1/3?fw=pt\n",
    "\n",
    "Transformer models are used to solve all kinds of NLP tasks. The following is a list of common NLP tasks, with some examples of each:\n",
    "\n",
    "- **Classifying whole sentences**: Getting the sentiment of a review, detecting if an email is spam, determining if a sentence is grammatically correct or whether two sentences are logically related or not\n",
    "- **Classifying each word in a sentence**: Identifying the grammatical components of a sentence (noun, verb, adjective), or the named entities (person, location, organization)\n",
    "- **Generating text content**: Completing a prompt with auto-generated text, filling in the blanks in a text with masked words\n",
    "- **Extracting an answer from a text**: Given a question and a context, extracting the answer to the question based on the information provided in the context\n",
    "- **Generating a new sentence from an input text**: Translating a text into another language, summarizing a text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7c7e00",
   "metadata": {},
   "source": [
    "#### Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3f1988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.5.1-py3-none-any.whl (431 kB)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.2.2-py3-none-any.whl (69 kB)\n",
      "Collecting transformers[sentencepiece]\n",
      "  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\anaconda\\lib\\site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: aiohttp in c:\\anaconda\\lib\\site-packages (from datasets) (3.7.4)\n",
      "Requirement already satisfied: packaging in c:\\anaconda\\lib\\site-packages (from datasets) (20.9)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-9.0.0-cp39-cp39-win_amd64.whl (19.6 MB)\n",
      "Collecting dill<0.3.6\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp39-cp39-win_amd64.whl (29 kB)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\anaconda\\lib\\site-packages (from datasets) (2022.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\anaconda\\lib\\site-packages (from datasets) (1.22.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\anaconda\\lib\\site-packages (from datasets) (2.27.1)\n",
      "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n",
      "Requirement already satisfied: pandas in c:\\anaconda\\lib\\site-packages (from datasets) (1.4.2)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py39-none-any.whl (132 kB)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: filelock in c:\\anaconda\\lib\\site-packages (from transformers[sentencepiece]) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\anaconda\\lib\\site-packages (from transformers[sentencepiece]) (2022.3.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\anaconda\\lib\\site-packages (from transformers[sentencepiece]) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: protobuf<=3.20.1 in c:\\anaconda\\lib\\site-packages (from transformers[sentencepiece]) (3.19.1)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91\n",
      "  Downloading sentencepiece-0.1.97-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\anaconda\\lib\\site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\anaconda\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in c:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (3.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\anaconda\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\anaconda\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: dill, xxhash, tokenizers, responses, pyarrow, multiprocess, huggingface-hub, transformers, sentencepiece, datasets, evaluate\n",
      "Successfully installed datasets-2.5.1 dill-0.3.5.1 evaluate-0.2.2 huggingface-hub-0.10.0 multiprocess-0.70.13 pyarrow-9.0.0 responses-0.18.0 sentencepiece-0.1.97 tokenizers-0.12.1 transformers-4.22.2 xxhash-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c31667",
   "metadata": {},
   "source": [
    "### Working with Pipelines \n",
    "    The Most basic Ojects in the Transformers library is the pipline() function. It connects a model with its neccesary preprocessing and post processing steps, allowing us to directly input any text and get an intelligible answer\n",
    "    \n",
    "   **pipeline is a short cut instead of doing preprocessing steps, directly we are using pipeline models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cb754d",
   "metadata": {},
   "source": [
    "## Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a456914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23495467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9450720548629761}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I have been waiting for a HuggingFace course my whole life\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c5b563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9990474581718445}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Edureka instructors was very good explaining in a understandable way\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87cb2a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9977620840072632}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"The movie was excellent but it is too lengthy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7da2cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9944183826446533}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Modi is developing Gujarat state comparing to other states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d65a0638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.615805983543396}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Modi is developing only Gujarat state comparing to other states\")  # above statement observe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5dbf7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9317912459373474},\n",
       " {'label': 'POSITIVE', 'score': 0.9975851774215698}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier([\"Modi developing only one state in all of 28 states\",\"Gujarat state is developing\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa53860a",
   "metadata": {},
   "source": [
    "### What is a Pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbadca27",
   "metadata": {},
   "source": [
    "By default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English. The model is downloaded and cached when you create the classifier object. If you rerun the command, the cached model will be used instead and there is no need to download the model again.\n",
    "\n",
    "There are three main steps involved when you pass some text to a pipeline:\n",
    "\n",
    "- The text is preprocessed into a format the model can understand.\n",
    "- The preprocessed inputs are passed to the model.\n",
    "- The predictions of the model are post-processed, so you can make sense of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfdabdc",
   "metadata": {},
   "source": [
    "- Some of the Currently available pipelines are :\n",
    "    - **feature-extraction(get the vector representation of a text)**\n",
    "    - **fill mask**\n",
    "    - **NER(Name Entity Recognition)**\n",
    "    - **Question-Anwersing**\n",
    "    - **Sentiment- Anaysis**\n",
    "    - **Summarization**\n",
    "    - **Text - Generation**\n",
    "    - **Translation**\n",
    "    - **Zero-shot-classification**\n",
    "         -  Let's have a look few of these"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6808d93b",
   "metadata": {},
   "source": [
    "### Zero Shot Classification\n",
    "    - We’ll start by tackling a more challenging task where we need to classify texts that haven’t been labelled. This is a common scenario in real-world projects because annotating text is usually time-consuming and requires domain expertise. \n",
    "\n",
    "      - For this use case, the zero-shot-classification pipeline is very powerful: it allows you to specify which labels to use for the classification, so you don’t have to rely on the labels of the pretrained model. \n",
    "\n",
    "     - You’ve already seen how the model can classify a sentence as positive or negative using those two labels — but it can also classify the text using any other set of labels you like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a1cc72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to roberta-large-mnli and revision 130fb28 (https://huggingface.co/roberta-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2febb77dfcaf46a6945dd838db5502e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model roberta-large-mnli with any of the following classes: (<class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSequenceClassification'>, <class 'transformers.models.roberta.modeling_tf_roberta.TFRobertaForSequenceClassification'>).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzero-shot-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\transformers\\pipelines\\__init__.py:702\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;66;03m# Infer the framework from the model\u001b[39;00m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;66;03m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;66;03m# Will load the correct model if possible\u001b[39;00m\n\u001b[0;32m    701\u001b[0m model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 702\u001b[0m framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    703\u001b[0m     model,\n\u001b[0;32m    704\u001b[0m     model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m    705\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    706\u001b[0m     framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[0;32m    707\u001b[0m     task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    710\u001b[0m )\n\u001b[0;32m    712\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    713\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mC:\\Anaconda\\lib\\site-packages\\transformers\\pipelines\\base.py:266\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    268\u001b[0m framework \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m framework, model\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model roberta-large-mnli with any of the following classes: (<class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSequenceClassification'>, <class 'transformers.models.roberta.modeling_tf_roberta.TFRobertaForSequenceClassification'>)."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e298e7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mclassifier\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is course about the Transoformer library  \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "classifier(\"This is course about the Transoformer library \",\n",
    "          candidate_labels = ['education','politics','business'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5806786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(\"Stephn Hawking said we can do time travel through black hole\",\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84385de",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(\" God Father movie is all about politics and Ghost movie is a suspense thriller movie both have good ratings,\n",
    "           \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecf6e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(\" This Diwali I should buy crackers in bulk but the where should I find whole sale shops?,\n",
    "           \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b365d8",
   "metadata": {},
   "source": [
    "- ** **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de446ed7",
   "metadata": {},
   "source": [
    "### Text Generation\n",
    "       - Now let’s see how to use a pipeline to generate some text. The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text. \n",
    "        - This is similar to the predictive text feature that is found on many phones. Text generation involves randomness, so it’s normal if you don’t get the same results as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa6c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a7e04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
